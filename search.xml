<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[魔兽争霸3重制版英雄图]]></title>
    <url>%2Farchives%2F1cbdb2f8.html</url>
    <content type="text"><![CDATA[魔兽争霸重制版来了，从官网盗了几张图~ 魔兽争霸重制版英雄图 网络盗图，图片来源: https://war3.blizzard.cn/introduce?type=story 真滴帅~]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习扫盲]]></title>
    <url>%2Farchives%2Fefc41243.html</url>
    <content type="text"><![CDATA[在工作中，经常遇到很多事情会涉及到算法机器学习，然而似乎有不少人会对算法亦或是机器学习有着这样那样的误解。于是我准备收拾一下，机器学习的基础知识，以辅助大家了解机器学习，知道机器学习是干什么的，它又是怎么工作的。🤗 机器学习扫盲什么是机器学习机器学习是人工智能的一种实现方式，机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。它致力于研究如何通过计算的手段，利用已知的经验来改善系统自身，从而应对新的情况时能够提供相应的判断。 简而言之，机器学习研究的是计算机怎样模拟人类的智力行为或者推测逻辑，以使得计算机获取新的知识或技能，并通过学习不断改善自身。计算机本身不具备推理能力，只有人类设计好告诉它如何学习，它才能去学习总结，了解因果关系才具备了推理能力。 举个简单的例子:人类是如何买西瓜的？水果店老板总是会说西瓜保甜不甜不要钱🤥。但是人类还是得费尽心思去挑甜的西瓜，因为聪明的人类知道水果店肯定有不甜的西瓜。那么接下来就是人类学习的过程: 12345671. 人类会先随便买一个🍉，或者让店老板挑一个，然后回去尝一下，发现不甜*(采样)*2. 又去了那家店，自己一本正经再挑了一个🍉，然后又回去尝了一下发现居然是甜的!*(采样)*3. 结果发现，甜的瓜柄比较匀称，不甜的瓜柄比较畸形，是不是瓜柄畸形的都不甜? *(推理/学习)*4. 于是再去买瓜，假装自己是高手挑了一个瓜柄比较匀称的，然后回去一尝哇甜的!*(验证)*5. 再买了两个瓜，当然柄比较匀称的，然后!居然!不甜!*(验证)*6. 于是经过仔细发现，瓜柄比较匀称的瓜，如果纹路不清晰，也会不甜*(调特征/再学习)*7. 最后人类每次去买瓜都是买瓜柄比较匀称，纹路清晰的瓜，于是绝大情况下都是甜的*(上线)* 那么计算机是如何学习呢，其实过程是一样的，它也需要通过不断的观察学习才能总结出一套模型来应对没见过的新事物。只不过计算机比较笨，推理能力比人类差得多，它需要人类预先告知它要观察什么(特征提取)，如何观察(模型设计)，如何调整/反思自己(寻优过程)，如何学习总结。它还需要比人类需要更多的样本和学习的次数，来学习出其中因果关系。 1. 先买他1000个西瓜(有钱任性)，全部劈开，先尝好甜还是不甜*(标注)* 2. 记录所有西瓜的瓜柄和纹理，还有其他觉得可能会对甜度有影响的信息*(特征工程)* 3. 计算机经过几百次几千次迭代，找到特征与甜不甜的关系模型*(模型寻优)* 4. 然后总结出一套理论，瓜柄比较匀称，纹路清晰的瓜，是甜的*(模型收敛)* 5. 再买20个瓜，验证下*(模型验收)* 近年来互联网数据大爆炸，数据无论在丰富度和覆盖面还有复杂度上都远远超出人工可以观察和总结的范畴，而机器学习的算法能在海量数据中，挖掘出有用的价值/信息，给我们带来了前所未有的思考方式。 机器学习关注哪些事情一. 回归问题回归(Regression)任务是拟合两个或者多个变量之间的关系，比如回归出房屋面积变量与房价变量之间的函数关系。回归问题通常用来预测一个未知“值”，一般情况下是这个值是连续值，比如预测房价、未来销量等等，我们无法成为上帝知道“真实值”但是我们能够用历史数据去预测真实值，回归即是对真实值的一种逼近预测。 举个简单的例子这里有一份某城市的房价数据如下(横轴为房屋面积，纵轴为房价): 我们希望预测不同的房屋面积的话房价大约是多少(目标是预测房价)那么我们第一反应就是想办法用现有的数据点，拟合出一根直线，利用这根直线我们就能够知道任意面积下的房屋价格是多少。那么拟合结果会是这个样子 以上过程就称为:**回归问题** 那么该函数就是对这些房价数据进行的一次回归也称线性回归(回归结果是一根直线)。此时利用这根直线就可以预测任何面积下的房价了。 但是，有网友就会有疑问了，这根直线拟合这么差😂，预测都不准的🙅‍♂️。所以我们才需要更好的回归模型，更多的特征变量(上述只有房屋面积这一个特征)，才能做出更好的拟合，才能更准的预测出房屋的价格。 另外，这里插一段“过拟合”的描述。如下图。其中蓝色直线为线性拟合，拟合程度肉眼可见的不够，很多点都离的比较远，像这种情况我们称为“欠拟合”其中橙色曲线为5阶拟合，感觉拟合不错，而且曲线也较简单其中绿色直线为15阶拟合，拟合程度相当给力几乎大部分点都在线上，但是！该曲线太过复杂，假设有测试集的话一定在测试集中表现相当不好。像这种情况我们称为“过拟合” 无论是过拟合还是欠拟合，都不是我们希望发生的，都需要尽量避免的情况。 二. 分类问题分类(classification)任务，是机器学习中最为常见的问题。她是在已标签数据样本中建立数据与标签的关联关系—分类器，以将未知标签的数据 分类 至确定的标签下。是一种有监督的学习。生活中常见的分类任务有，垃圾邮件判别，肿瘤判断，恶意评论分类，植物科目分类，情感分析等等。 常用的分类模型包括: 朴素贝叶斯，逻辑回归，KNN，决策树，SVM，神经网络 接下来举一个分类的栗子🙋‍♂️: 我们有一篮子的水果我们并不认识它们，只是知道里面有两种水果被称为‘苹果’和‘柠檬’。 我们希望能够分类出篮子里面所有的水果(目标是水果分类) 那么，我们先麻烦一个有知识的人去将一个篮子里面的水果标记为‘苹果’和‘柠檬’，我们需要利用这个篮子里面的已标记水果进行模型训练，再想办法用这个模型去分类那些没有标记的篮子。于是，我们挑选了水果的长和宽作为特征。如图: 那么接下来的分类任务，就是找一根直接(线性分类)，将这两类水果分离开。那么怎么找呢，就是上面介绍到的，逻辑回归模型啦，决策树模型啦等等。比如，我就用两个两种不同的模型做了两种不同的结果。 寻找这根直线的过程，称为学习过程。 紧接着我们就可以利用这个已经学习好的直线(分类器)，将未知的水果进行分类了。方式也很简单，就这个例子来说，落在直线左侧的未知点(图中的空心点)都被预测为‘柠檬’，对应的落在直线右侧的未知点(图中的空心点)都被预测为‘苹果’: 分类问题番外 不看也没关系，就是想写🤪 分类(classification)任务中有几点是确定的非常重要的: 首先她有一个确定的离散的标签集合 其次她是一个有监督的学习，所以需要有预先的标注好的样本数据 标注的数据与预测的数据，两者的范围与分布是一致的 在工作中会有一些没有接触过机器学习分类的同学，会不经意破坏上述几点，给模型的训练和建立带来了不小的挑战。(如果大家有兴趣，当真破坏了上述某一项会发生什么，又应该如何应对，请给我留言) 线性分类/非线性分类在分类的模型中，根据分类的参数是否是线性函数(即是否具备线性分类面)来区分线性分类与非线性分类。另外，根据分类的训练目标不同又有生成模型和判别模型的区别。 生成模型/判别模型根据分类器的思路不同角度不同而区分出的两种不同的模型类型: **生成模型**:主要根据一类样本本身的特征生成一个模型，最终会为每个类别都生成一个单独的模型。准备分类时，将样本特征输入每个模型中得到该样本是该分类的概率，最终取概率最高的分类。 生成模型更关心每个类目的整体分布和类目中心。 **判别模型**:主要根据类别之间的区别，利用区别显著的特征生成一个模型，最终只有一个模型。准备分类时，将样本特征输入该模型，就能得到具体的分类结果。 判别模型更关心每个类目的边界。 三. 聚类问题聚类任务(Clustering)是将未知类目的数据，将彼此相似的对象聚集在同一个簇中。聚类分析是把一组数据按照相似性和差异性分为几个类别，其目的是使得属于同一类别的数据间的相似性尽可能大，不同类别中的数据间的相似性尽可能小。一般情况下，聚类任务都事先设定好聚成几个簇，但是不会知道这些簇的含义。甚至同一种算法，跑几遍都能够聚出不同的结果。 常用的聚类模型包括: Kmeans，K-MEDOIDS，层次聚类 聚类能够在我们无法预先知晓类目的情况下，将一些群体进行聚集，类似于客户群体分类，市场细分等等。 以下是一个聚类成三个类的栗子(随便取的数据): 一般情况下，聚类的结果就是一个个簇，如果新的未知点进来，需要计算与每个簇的距离，再判断它属于哪个簇。 聚类任务因为没有确切的分类目标，因此会导致，同一份数据，同一个算法也有可能出现，不同的聚类结果: 四. 异常监测异常(值)监测，是将一批数据中，发现不合群的异常值。 常用的异常监测模型包括: $3\sigma$准则，大标准残差检验法(Grubb检验)，iForest，DBSCAN(转化) 之前在工作中，我们就希望在所有店铺接单率中发现一些异常的接单率下降。每个店铺每个小时甚至每10分钟都有实时的接单率数据产生；因为线下实际业务的关系或许太忙或许没看到等原因会正常拒单，但是店铺接单率通常会保持在一个正常的水平。如果发现接单率突然下降或者低的太异常我们就需要预警，可能设备或者软件等发生了问题；难点在于每个店铺情况都不一样甚至不同时间段也不一样，很难设置一个简单的阈值规则来预警，于是我们需要在茫茫多的接单率的数据中找出异常的接单率，来帮助我们预防异常情况。 以下是发现异常值的一个简单的例子: 五. 数据降维数据降维，是指采用某些隐射方法，将原先高纬度的特征数据，降低其数量，使之得到低纬度的一组不相关主要特征。降维的思想前提是高纬度的特征一定存在一些相关的冗余信息，或者一些毫无用处的噪音信息，并通过一些算法的方式将数据中的核心信息，本质信息提取出来。 常用的数据降维模型: 主成分分析(PCA)，线性判别分析(LDA) 机器学习基本流程1. 总体流程机器学习过程，总体分为以下四个步骤 (1)、特征提取 也称为数据准备，包括样本的采集，异常数据的清洗，等等内容。直接面对的是数据源，比如数据爬虫，日志导出，数据库获取等等。包含取数的手段，数据采样，清洗等。 (2)、特征工程 也称为数据预处理，将原始数据加工成模型所需要的，规范化的，表达能力强的特征。包含特征处理，特征降维等。 (3)、模型训练 顾名思义，这个过程是为了生成一个具体的模型，其中就包含模型选择，调优等等。 (4)、结果评价 每种任务必须有自己的模型评价标准，甚至还有具体的业务标注。有了评价体系我们才能判断模型的好坏。包含准确召回，F1值，AUC等等。 (5)、反馈 整个流程不应该只有一个正向链路，更应该有一个反馈方向。如下图。当模型评价模型不给力时，需要反馈模型调整参数使之更加拟合，达到更好的效果。但模型已经拟合很好了，依旧达不到要求的效果。那就需要进一步进行更多的特征挖掘，挖掘出更多有用的特征，以帮助模型提升上限。如果这些都无法达到要求，就要考虑是否应该获取更多的数据和更多的样本，来帮助整个模型进行提升。 2. 特征提取&amp;特征工程 (1)、特征提取特征提取，也称为数据准备，是直接面对原始数据的部分。其中包含了样本的采样处理，异常值的清洗，一些原始特征的提取。 (2)、特征工程特征工程，也称为数据预处理，包含着特征三部曲: 特征处理，特征选择，以及特征降维。特征工程是整个机器学习最最核心的部分，特征提取是模型上限的制定，在后续过程中再好的模型再好的优化都只是为了接近这个上限而已。 (3)、模型训练模型训练，就是算法工程师经常面对和处理的，包含模型的选择，调参等等。为了将模型更好的拟合上述数据。 (4)、结果评价结果评价，包含模型的评价和业务指标的评价。在实际工程中，模型的指标并且能够直接代表业务的目标。因此我们可能需要更加重视业务指标的评价带来的影响。 机器学习的应用领域疑问答疑(1) 那么机器学习算法与算法导论的算法有什么区别？有时候会有人疑问，机器学习算法与我们平时接触的排序算法，最短路径，动态规划算法有些什么不同？实际，他们本质上都是解决问题的一系列步骤。从这点出发他们都是一致的。算法导论里的算法本质上是对有精确解的问题，如何更有效率地求得这个解。这个效率可以是计算时间更短，也可以是计算过程所需要的空间更少。机器学习要解决的问题一般没有精确解，也不能用穷举或遍历这种步骤明确的方法找到解，而且需要强调的是“学习”这个属性，即希望算法本身能够根据给定的数据或计算环境的改变而动态的发现新的规律，甚至改变算法程序的逻辑和行为。机器学习里面大部分想要解决的问题是NP困难的 (比方说最宽泛的优化问题）。这类问题一般会考虑用迭代求解的办法尝试寻找一个近似解。又或者有时候直接求解一个问题的复杂度会是O(n^3)但是迭代的话现实中会比理论的更快收敛(e.g. matrix inversion by conjugate gradients vs. by direct inversion)，算法导论里面的问题比如排序或者字符串处理并不在这一类NP困难的问题内, 因此迭代的方法可能不那么常见。因此，经典算法是在解决问题，然而学习算法中有些环节并不是在解决问题，而是在猜测问题，有了对问题的猜测，才开始解决问题，不同的学习算法就包含了不同的猜问题的策略，也就是归纳偏执（inductive bias）。这一部分就是经典算法中所没有的部分，是机器学习研究的核心。 (2) 分类与回归有什么区别同样都是预测，分类的目标类目是离散的，回归的目标值是连续的，这是最大的区别。 (3) 分类与聚类有什么区别分类有明确的分类类目，聚类只能设定聚成几类对于结果类目是未知的。分类一般是有监督的需要样本标注，聚类一般是无监督的无需标注的样本。]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征处理之(四)：缺失值和组合]]></title>
    <url>%2Farchives%2Fa06283d5.html</url>
    <content type="text"><![CDATA[数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！很多情况下，数据总有可能缺失，但是若是放任不管可能造成严重的后果甚至模型无法收敛，所以我们得利用已有的信息进行数据填充来降低损失。特征组合是将不同的特征进行组合，生成新的特征，这种方式能提升模型更大的非线性能力和拟合能力。 特征处理之缺失值填充和特征组合变换缺失值填充一、为什么要进行缺失值填充数据缺失在许多研究领域都是一个复杂的问题。对数据挖掘来说，空值的存在，造成了以下影响： 模型会丢失很多有用的信息 使得模型挖掘过程中陷入混乱，导致不可靠输出 依赖模型本身，无法很好地处理缺失值的情况，导致不确定性 另外数据缺失也分为三类: 1) 完全随机缺失 2) 随机缺失 3) 非随机缺失 二、如何进行缺失值填充2.1 统计填充利用特征本身的数据进行对缺失值填充。思想是既然缺失了不知道他的值，那么就用最平常的值给它填不上，不要造成混乱，期望其他特征能表达出该样本的特性。 所以利用特征本身的数据，计算出平均值或者中位数，或者其他统计型的平常数据，来填充缺失值。 2.2 相似填充利用其他不缺失的特征，找到相似的样本，用相似样本的该特征来对缺失值填充。思想是用相似样本来仿照出来值会更加靠谱。 其中KNN方式填充，是比较常见的。利用非确实的特征，找到距离其最近的几个样本。利用这几个样本来帮助填充缺失值。 特征组合变换一、特征组合变换有什么作用通过对单独的特征进行变换和组合，能够得到新的特征，这些新特征自带非线性能力，提升了模型的表达，很大程度帮助提升结果。 增加模型的表达能力 变相提升模型非线性能力 得到更加有效的特征 二、单特征变换 x ==> \sqrt{x},\frac{1}{x},x^2,logx单特征变换，就如图一样，有几种简单的变换方式。形成新的特征。 三、多项式变换 a,b ==> 1,a,b,a^2,b^2,ab多项式变换是指两个或多个特征进行组合变换，生成高次特征。 结语特征处理是在机器学习中占据非常重要的地位，特征工程决定了整个模型的上限，而特征工程中最基础的就是特征处理。本篇主要回顾下特征处理中的缺失值填充和特征组合。最后最后拿出成果： 什么是缺失值填充/特征组合(这俩简单到不用解释了吧) 缺失值填充/特征组合有什么作用 缺失值填充/特征组合如何操作 附录特征处理PPT]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征处理之(三)：离散化]]></title>
    <url>%2Farchives%2Ff4d50b3c.html</url>
    <content type="text"><![CDATA[数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！离散化简化了模型训练的复杂性，降低模型过拟合的风险，对异常数据有更强的鲁棒性。另外离散化之后可以进行特征编码，提升训练时间，也能模型提升表达能力。是个非常重要的特征处理方式。 特征处理之离散化一、什么是离散化将连续型的特征进行离散处理，得到有限的离散值。 如图： 另外再借用哑编码的图片： 二、离散化有什么作用 简化了模型训练的复杂性，降低模型过拟合的风险（离散化带来的优势） 离散化后的特征对异常数据有很强的鲁棒性（离散化带来的优势） 稀疏向量内积乘法运算速度快，因为稀疏矩阵有实数的值很少，做内积运算时就会有很多优化手段 线性模型表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合能力； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力 红色部分是离散化带来的作用或者说优点，蓝色部分是离散化后的哑编码带来的作用和优点。 三、 如何离散化（无监督）假设有一串连续的数值，我们需要对其进行离散化：以下都会使用这份数据作为例子。 1例子: 1 3 4 5 5 6 7 7 8 10 11 13 15 3.1 等距/等频离散（1）等距离散用相等的距离作为范围，使得在同一范围内的数值，被认定为同一类别。 假设距离为5，那么就有A:(0,5],B:(5,10],C:(10,15]三个离散范围 于是，我们将上面的例子进行离散: 等距离散: 123范围选择 A:[1 3 4 5 5], B:[6 7 7 8 10], C:[11 13 15]例子: 1 3 4 5 5 6 7 7 8 10 11 13 15离散: A A A A A B B B B B C C C （2）等频离散用相等选择频度作为范围，使得在同一频度内的数值，被认定为同一类别。 假设距离为3， 于是，我们将上面的例子进行离散: 等距离散: 123步频选择 A:[1 3 4], B:[5 5 6], C:[7 7 8], D:[10 11 13], E:[15]例子: 1 3 4 5 5 6 7 7 8 10 11 13 15 离散: A A A B B B C C C D D D E 3.2 KMeans离散借助于KMeans聚类的思路，将数值进行聚类，最终聚为一类的则被认定为同一类别。 这里不太好举例子，咱们用数轴来表述，数轴上蓝色蛋蛋更大意味着数据在这里的数据越多。 然后咱们开始，K=3的聚类，首先随机散落三个点作为中心点 所有的数值，选择离散落的三个点最近的点作为第一次聚类结果 以聚类结果的中心，作为新的三个中心点，重新重复上述过程。 最终会收敛聚类结果不再变化。 四、如何离散化（有监督）有监督的离散化，主要利用了已有的类别标注来将特征进行更好地离散。为了表达方便，将上面使用的例子标注一行类目。 12例子: 1 3 4 5 5 6 7 7 8 10 11 13 离散: C1 C1 C2 C1 C1 C2 C1 C1 C2 C2 C1 C2 4.1 基于信息增益的离散这种方式是一种二分方式的离散方式，每一步只离散两份，然后再在这两份中用同样的方式进行离散，直到达到离散个数的目标或者层数的目标。 那么每一步是如何找到最佳的离散点的，就是基于信息增益率的最大化。 为每一个可能的离散点，都计算一次信息增益率，选择信息增益率最大的那个离散点，作为本次离散的点。 如何计算信息增益率，偷个懒直接截图PPT了，这里列出了信息熵到信息增益率的公式表达： 公式没有说明白，那么用一个例子来进行计算，如下图 信息熵: 因为有12个数据，其中有5个C2，7个C1，因此Y的信息熵为$H(Y)=0.9798$ 条件熵: 如果离散的分割线在红色位置，那么离散后红色之前都是X1，红色之后都是X2，因此条件就是X。因为X1有5个数据，因此$p_xi=\frac{5}{12}$，其中有1个C2和4个C1，因此$H(Y|X=X1)=-\frac{1}{5}log\frac{1}{5}-\frac{4}{5}log\frac{4}{5}$ 剩下的基本已经很简单了，可以直接自行计算。 4.2 基于Gini增益的离散与基于信息增益的离散，唯一的区别就是衡量方式由信息增益率变成了Gini增益。Gini增益的计算如下： 结语特征处理是在机器学习中占据非常重要的地位，特征工程决定了整个模型的上限，而特征工程中最基础的就是特征处理。本篇主要回顾下特征处理中的离散化。最后最后拿出成果： 什么是离散化 离散化有什么作用 如何进行无监督离散 如何进行有监督离散 附录特征处理PPT]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征处理之(二)：无量纲化和哑编码]]></title>
    <url>%2Farchives%2F4cc8a378.html</url>
    <content type="text"><![CDATA[数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！无量纲化能提升特征可比性，优化训练时间；哑编码能降低模型复杂度，提升特征表达能力，提升运算时间，提升非线性能力。 特征处理之无量纲化和哑编码无量纲化一、什么是无量纲化首先，什么是量纲，量纲是指：将一个物理导出量用若干个基本量的乘方之积表示出来的表达式，称为该物理量的量纲式，简称量纲。举个简单的例子，“身高=1950mm”，这里面其实蕴涵着两样东西，一个是1950 是数字基本量，另一个是 1cm是单位基本量 ，所以 $1950mm = 1950 \times 1mm$ 就是一个量纲式。另外相似的，“5平方米”，就是$5 \times 1m \times 1m$ 也是一个量纲式。 那么量纲会带来一个问题，就是无法进行相互比较，身高1950mm的人，与身高20cm的人谁更高？1950&gt;20？于是1950mm的人更高吗？因此特征与特征之间因为量纲的存在导致数字量与数字量之间无法比较大小，也就在最后的时候无法判断特征与特征之间的重要性对比。比如模型 $y=\theta_1x_1+\theta_2x_2+ \theta_3$，其中 $\theta_1$ 就是特征值 $x_1$ 的参数也就意味着 $\theta_1$ 就是特征(身高)的重要性。但如果如下左图 $x_1$是身高， $x_2$是臂展，$\theta_1$需要除以10才能与 $\theta_2$对比大小，否则就不公平。 于是我们需要去除特征值的量纲，那么去除量纲也有两种不同的方法，一个叫归一化，一个叫标准化。两者操作很像，但作用和目的不同。 二、无量纲化有什么作用标准化的目的是将样本的各个特征值转换到同一量纲下使得不同度量的特征具有可比性； 归一化的目的是将各样本转化为单位向量使得模型迭代更快更好； 三、无量纲化如何操作3.1 标准化数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。标准化面向的是一个特征，对该特征的所有样本值进行标准化。 （1） Min-Max 标准化 x'= \frac{x-min}{max-min}Min-Max 标准化，是可以将数据从自然范围缩放至0-1的区间，且不破坏分布情况。 （2） Z-score 标准化 x'=\frac{x-\mu}{\sigma}Z-score 标准化，是可以将数据从自然范围缩放至较小的区间，且满足均值为0，方差为1（注意并不是将分布变成了标准分布，只是满足了标准分布的均值方差条件）。也不会破坏原有的分布情况。 （3） Min-Max 与 Z-score 标准化的异同 异：Min-Max变换依赖某两个值最大和最小，Z-score标准化则依赖所有值 异：Min-Max会转变成[0-1]的区间内，Z-score标准化没有这个区间限制只是数据的均值为0方差为1 同：标准化目的都是为了使得不同度量的特征具有可比性 3.2 归一化数据的归一化也是将数据按比例缩放，使之落入一个小的特定区间。但归一化面向的是一个样本，对该样本的所有特征值进行归一化。 （1）为什么要归一化 提升模型收敛速度 提高模型算法精度 归一化效果如下图，归一化之后等高线更圆更均匀（如右图），在似然函数寻优时就能提升模型收敛速度（图中红线为梯度寻优）；提高算法精度是因为在计算一些距离的算法时，下图中变量$\theta_1$的取值范围比较小，涉及到距离计算时其对结果的影响远比变量$\theta_2$带来的小，所以这就会造成了精度的损失。 所以归一化很有必要，他可以让各个特征对结果做出的贡献相同。 （2）基于L2的归一化 x'=\frac{x}{\sqrt{\sum_{j}^{m}{x_j^2}}}基于L2的归一化，是将数据除以L2范数，就是各元素的平方和然后求平方根。 3.3 标准化与归一化的区别 对象不一样: 标准化的对象是一个特征列； 归一化的对象是一个样本行; 目的不一样: 标准化的目的是将样本的各个特征值转换到同一量纲下使得不同度量的特征具有可比性； 归一化的目的是将各样本转化为单位向量使得模型迭代更快更好； 哑编码一、什么是哑编码首先哑编码面向的是离散型的特征。哑编码是将一个离散型特征进行一对多映射产出多个特征的编码方式，每个特征编码只代表一个若干级别间的差异。 下图即为年龄特征，离散化成为年龄段特征(有单独的另一篇文章讲述)，再最后哑编码为四个不同的特征。 二、哑编码有什么作用 简化了模型训练的复杂性，降低模型过拟合的风险（是离散化带来的优势） 离散化后的特征对异常数据有很强的鲁棒性（是离散化带来的优势） 稀疏向量内积乘法运算速度快，因为稀疏矩阵有实数的值很少，做内积运算时就会有很多优化手段 线性模型表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合能力； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力 三、哑编码如何操作3.1 oneHot 编码One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。 3.2 dummy 编码和One-Hot编码基本一样，但是比One-Hot编码少了一个状态。其实可以简单理解，“少年”这个状态被其他状态全为0给取代了。 3.3 One-Hot 与 dummy 编码的区别如果不使用正则化的情况下，那么One-Hot编码的模型会有多余的自由度。这个自由度体现在你可以把某一个分类权重参数增加某一数值，同时把其余的分类权重参数都减小某一数值，而模型不变。在dummy编码中，这些多余的自由度就都被限制住了。 可以简单用 $y=\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4+\theta_5$ 作为例子，${x_1,x_2,x_3,x_4}={少年,青年,中年,老年}$ ，当模型的$\theta_5$下降时(无论是减小还是缩小)，其余的所有$\theta$都对应增加(或者变大)，就可以保证模型不变了。这就是模型的自由度，会在训练或者调优时造成麻烦。 但如果使用正则化的情况下，那么正则化本身就能够处理这些多余的自由度。此时，用One-Hot编码看上去更有优势，因为每个分类型变量的各个值的地位就是对等的了。 结语特征处理是在机器学习中占据非常重要的地位，特征工程决定了整个模型的上限，而特征工程中最基础的就是特征处理。本篇主要回顾下特征处理中的无量纲化和哑编码。最后最后拿出成果： 什么是无量纲化/哑编码 无量纲化/哑编码有什么作用 无量纲化/哑编码如何操作 附录特征处理PPT]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征处理之(一)：背景知识]]></title>
    <url>%2Farchives%2F45aaf959.html</url>
    <content type="text"><![CDATA[数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！ 特征处理之背景知识在机器学习过程中有一个特别重要的环节那就是特征工程(Feature Engineering)。特征工程 得到的特征决定了一个机器学习任务的最终上限。试想一个上限不高的机器学习任务，再厉害的算法策略又能怎样。又再想一下，上限非常高的机器学习就算再朴素的模型不断调参尝试后，总归能不断获得进步(因为有很高的上升空间)。因此在做机器学习任务的时候可能大部分时间精力需要投入到特征工程中，可见特征工程的重要性。 特征工程（Feature Engineering）泛指从原始数据转换为特征向量的过程。其中包含三大模块: 特征处理，特征选择，特征降维。 本篇主要记录下特征处理的背景知识先再最前面预留一些问题，如果这些问题都能回答上来那么本篇文章基本也就不用看了省点时间多睡觉 什么是特征 为什么要进行特征处理 特征处理在机器学习过程中处于哪个位置 特征处理与特征工程是什么关系 特征有哪些类型 要如何进行特征处理(在其他”特征处理”文章中细讲) 以下我自己对这些问题的理解，不对请务必要喷，被喷使我成长 o(￣▽￣)ｄ 一、什么是特征在机器学习任务中，描述/表达一件事物的特性属性数据/抽象结果，称为特征。 记得初学的时候，觉得收集的数据不就是特征么。其实这个理解不能说不对，但太浅显，特征是直接作用于我们最终分类(聚类回归等)任务的，为了更好地服务最终任务，特征承载了更多的意义，它需要为了帮助机器的最终任务更好地去描述和表达一件事物。 稍微通俗一点，假设收集了NBA球员的身高数据准备做一些有趣的事情，那么身高数据本身就是一个原始特征，但因为数据连续样本不够等等原因难以很好的利用起来。那么如果将身高数据换算成适当的身高区间数据呢？是不是就能够更好的完成那件有趣的事情了。 二、为什么要进行特征处理当前数据/特征表达能力不够好或者不能达到模型要求，因此要进行处理。 比如拿到的是图片或者多媒体数据，如果不将原始数据处理成更具有表达性的特征数据，直接处理那些原始数据会造成一系列问题，比如算力不够、特征稀疏等等。再比如为了提升迭代速度，或者评判特征之间的重要性等等原因，也需要对数据进行特征处理。 上述例子收集了NBA的身高数据准备做一些有趣的事情。上面说的将身高换算成适当的身高区间就是一种特征的处理，使得区间内部的身高数据都不会造成不同的影响，但区间与区间之间却有着完全不同的表现，将身高特征的表达能力变得更好更合理了。另外如果发现收集到的数据有一些NBA人员的身高缺失了？有一些人员的数据是异常的？备选的训练模型还不接受缺失值？也是需要特征处理来解决问题。 三、特征处理在机器学习中处于哪个位置3.1 特征处理知识框架—机器学习过程机器学习过程，以我自己的理解，分为了四个部分，从上至下。 特征提取: 主要是从一些数据源提取到一些原始特征(也可以叫原始数据)。 特征工程: 将上述原始特征，加工成为模型所用的规范化的表达能力强的特征。 模型训练: 生成/优化模型，使模型不断拟合上述特征数据 结果评价: 对上述产生的模型进行标准的评价和衡量，为优化模型或者特征给出指导意义。 3.2 特征处理知识框架—特征提取和特征工程 特征提取特征提取包括样本的采集，取样方式，异常数据的清洗等等内容，直接面对的是数据源，比如数据爬虫，日志导出，数据库获取等等。 特征工程特征工程就是在特征提取的基础上进行加工处理。传说中特征工程有三部曲，就是指: 特征处理 特征选择 特征降维。 本篇以及后续篇章着重的就是第一曲——特征处理。终于引出了文章中心。🤔 特征处理大致包括了对各种原始进行各种计算或者操作，使之更加规范，表达能力更强。特征选择，顾名思义就是在对特征处理之后，通过某些手段，选择对当前模型帮助最大的几个特征来。特征降维，是指面对维度很高的特征时对训练模型会带来很大压力，于是在保证特征表达能力的情况下，降低特征的维度来提高训练的速度和精度。 3.3 特征处理知识框架—再说特征工程 再说特征工程拿出特征工程再细分聊下，在特征工程中，包含了很多处理方法。 无量纲化: 将特征去除量纲，使特征可对比可衡量 离散化: 将连续型特征进行分组，使之离散，比如年龄特征处理成年龄段 哑编码: 对离散的特征进行编码化 缺失值填充: 某些特征数据丢失的情况下想办法填充一个值 组合变换: 单个特征表达能力也许不强，但是经过组合后，可能就很强，提升了模型的非线性能力。 3.4 特征处理知识框架—模型训练和结果评价 模型训练模型训练中，也有大致如下这些步骤: 模型选择：面对一个特定的机器学习任务应该如何选择合适的模型。 参数调整：在训练样本中将模型调整至最优。还记得这个最优的上限是哪儿。 组合模型：在一个特定的任务中将多个模型组合在一起共同完成任务。 结果评价当模型训练好之后，必须要训练好的模型，进行测试评价。才知道这个模型泛化能力行不行: 给它一波训练集外的新数据它能不能也给出很好的结果。 一般情况下，样本数据都会分成训练集和测试集。训练集用来训练模型，训练后使得模型对这些训练样本达到很好的效果(称为模型拟合了训练集)。但真正看模型是不是好的，不是看它对训练集达到了多高的效果，而是看它对测试集(从未训练过的数据)的效果如何。 如果对训练集本身就效果不好，称为“欠拟合”如果对训练集效果很好，但是用测试集就效果很差，称为“过拟合” 另外，有一些衡量模型的指标，比如“准确率”，“召回率”，“F1值”，“ROC”，“AUC”等等。 四、特征有哪些类型 4.1 连续性区分连续性区分方式中，会有几种基本的分类。 连续型特征: 比如年龄，身高，温度等等这些连续数值型的特征 离散型特征: 比如胖-中-瘦，比如颜色，比如花的种类等等这些有限可枚举的特征 二值型特征: 是离散型的特殊情况，只有两个值。比如是否患有癌症。 4.2 来源区分来源区分的方式，我是来凑数的，四个比三个更平衡嘛。😂其实也稍微有点道理的: 数值型特征: 我们常见的年龄，身高等等 文本特征: 在文本中才有的一些特征，比如词频，词向量，TF值等等 图像/音频特征: 在图像/音频中才有的特征，比如rgb图像矩阵，灰度值，锯齿，轮廓等等。 4.3 复杂性区分 低阶特征: 一些初始的特征，比如统计值等等 高阶特征: 在一些初始的特征的基础上，挖掘、分析、组合来得到对模型更好的特征 4.4 动态性区分 稳定特征: 一般情况下不会变化的特征，比如人的肤色，汽车马力等等 动态特征: 会随着时间空间变化的特征，比如所在地理坐标，噪音值，天气等等 结语特征处理是在机器学习中占据非常重要的地位，特征工程决定了整个模型的上限，而特征工程中最基础的就是特征处理。本篇主要分享下，特征处理的背景知识，包括特征的概念，机器学习的框架，以及特征处理在其中的位置，和特征的某些分类。 最后最后，再把这些问题抛出来，是否都已经讲清楚了呢 什么是特征 为什么要进行特征处理 特征处理在机器学习过程中处于哪个位置 特征处理与特征工程是什么关系 特征有哪些类型 附录特征处理PPT]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文切词方法学习]]></title>
    <url>%2Farchives%2Fd5955c4c.html</url>
    <content type="text"><![CDATA[前段时间有小伙伴问了我好几次中文切词是怎么切的，我想着应该是我没表达清楚所以小伙伴才会重复问多次，是我表达能力不够，临时说也说不上来。所以就写一篇了简单的中文切词方法的短文，一方面是锻炼下自己的表达能力，另一方面下次小伙伴再问就直接扔给他Y(^o^)Y 本文简书链接 中文分词介绍在文本处理中，如果需要理解分析句子背后的含义(语义)，或者需要加入人的一些词语的先验知识，又或者数据量不够大细粒度的单元(字)容易导致过拟合(字的字典表一般都比较小)，或者有些特殊的只适合词的模型的一些场景下都需要 切词 来将句子切分成词的有序串。特别的，“词”是表达/承载语义的最小单位，在需要表达句子乃至文章意义的时候切词是必要的。但并不是所有文本处理都需要/适合切词，在数据集比较足够大的情况，或者一些深度网络模型，或者机器翻译的场景中，都是字粒度表现的更好。所以是否真的需要切词，是在不同的场景下不同的数据下需要分别对待的。在不是必须要分词的情况下，优先尝试使用字粒度的模型能够更快更便捷的不错的结果再考虑是否引入词/语音的概念。 在英文中每个词本身就有分割的标记，但是在中文当中词与词之间没有明显的区分标记，而且经常会有字的前后语境不一样切分方式就差别很大的情况。因此如何切词(切词算法)就显得额外重要。下面以我自己的理解和简单的语言浅显地分享下我个人理解的各种分词策略。 中文切词分类在中文切词中大多数文章都会分为三类:1. 基于词典的分词方法(也有文章称”基于字符串匹配方法”)2. 基于统计的分词方法3. 基于序列标注的分词方法 第一类基于词典的分词方法扫描方向不同：正向/逆向最大匹配法、双向匹配分词法。长度倾向不同：最少切分算法。它们最重要的思想就是按照一定策略将待分析的字串与一个大词典中的词条进行匹配，若字串在词典中存在则匹配成功，然后进行下一个字串的猜想。 第二类基于统计的分词方法N元模型(N-gram)、Aprior算法(频繁项集算法)他们最重要的思想就是在大量文本中，某些相邻(有顺序)的字(/词)同时出现的次数越多，就越可能构成一个词(/词串)。因此这些算法是统计或者分析字与字顺序相邻出现的概率来得到最大概率的切词结果。 第三类基于序列标注的分词方法条件随机场(CRF)、隐马尔科夫模型(HMM)、最大熵算法等当然也包括神经网络分词模型。他们都属于一种字构词方法，最重要的思想是把分词过程视为字在字串中的标注问题（例如将字标注为“首字中间字尾字”或者其他标注方式）。当这些标注完成的时候切词也就自然完成了。这种策略能够平衡地看待词表词和未登录词(未收录到词典的词)的识别问题，大大简化了使用门槛，并得到一个相当不错的切词结果。 *强行插入一波广告隐马尔科夫模型的浅显介绍链接 第四类其他分词方法分词方法不限于上述几种方式和算法。还有很多有效可靠的算法，我没有很好地将他们分类。包括N-最短路径算法，自动感知机算法，基于字的生成式模型，等等。另外还有通过计算机模拟人类对句子的理解，同时具备句法的分析，语义的理解，并共同作用于分词的结果而且能顺带给出歧义的最优解。还有一个最准最强分词器(性能差了点)——人脑。小伙伴们有没有发现人脑能简单地进行分词，机器却这么复杂和困难才勉强能使用，人脑还是hin强大啊。 以上就是基本分词算法的分类以及其主要的思想。 下面分享一些基本的算法理论介绍，觉得无趣的小伙伴先去洗洗睡吧。 正向/逆向最大匹配算法先说说最大匹配算法，最大匹配算法就是按照某种策略使得切得的词包含的字串长度最大。正向最大匹配算法是从左往右进行匹配，反向则是从右往左进行匹配。举个栗子: 词典: 研究，研究生，生活，水平，活水，的确，确实，在理。句子: 研究生活水平；他说的确实在理。 那么正向匹配算法，匹配过程：(咱们字典最大长度为3)从左往右先选择最大的长度作为待查字串，然后查询字典，没命中的情况下，将最后一个字去除，再查询字典，如此往复，直到待查字串为空。第一句: 研究生活水平 待查字串 是否命中字典 输出 研究生 是 研究生 活水平 否 活水 是 研究生 活水 平 否 null 研究生 活水 平 下一句: 他说的确实在理。 待查字串 是否命中字典 输出 他说的 否 他说 否 他 否 null 他 说的确 否 …略 否 他 说 的确在 否 的确 是 他 说 的确 实在理 否 实在 是 他 说 的确 实在 理 否 null 他 说 的确 实在 理 那么逆向最大匹配算法，也就是说是个从右往左的一个反向过程，实际上与上述过程一样。只写一句栗子。 待查字串 是否命中字典 输出 实在理 否 在理 是 在理 的确实 否 确实 是 确实 在理 他说的 否 说的 否 …略 否 他 说 的 确实 在理 小伙伴也看到了，逆向匹配好像要优于正向匹配。也确实实验表明(哪里有过统计)，逆向最大匹配算法要优于正向最大匹配算法的。但也不绝对，还是有不少句子正向的结果要比反向结果要好很多的。 双向最大匹配算法双向最大匹配算法是将正向最大匹配法与逆向最大匹配法组合。将句子分别使用正向最大匹配和逆向最大匹配法进行扫描切分。如果两种分词方法得到的匹配结果相同，则认为分词正确，否则，按照最小集或者其他策略进行选择和融合处理。据说，中文中90.0％左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0％的句子两种切分方法得到 的结果不一样，但其中必有一个是正确的(歧义检测成功)，只有不到1.0％的句子，正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对(歧义检测失败)。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。 最少切分策略最少切分策略，思路也比较简单，将几种切分算法得到的结果，选择词数最少的，如果词数一样再用其他方式进行挑选。这个只是个挑选策略，事实证明句子词数最少的切分方式往往都是正确的。 N元模型(N-gram)切词基于N元模型的切词策略是将训练好的N-gram模型进行路径计算得到最优切分策略路径。巨大多数句子由于歧义的存在，肯定会有多种切分路径，之前的最大匹配算法就是就是用一种贪婪的规则方式选择最优的路径，N-gram模型则是用统计得到的先验概率来得到概率最大的路径。依然还是举上述的例子: 句子: 他说的确实在理假设我们使用的2-gram，每个字出现的概率只与其前面的一个字有关 那么我们能举例出所有的词路径： 找出其最优的路径，就能得到最优的切分方式。在其他算法中如最短路径算法，是设定每个路径上的每条边作为一个触达距离，然后再使用Dijkstra或者其他最短路径法来得到最短的触达距离作为最优解。在N-gram模型的算法中，每个路径上的边都是一个N-gram的概率于是得到如下概率路径有向图。 那么分词任务就转变为如何求解最优策略路径假设随机变量S为一个汉字序列，W是S上所有可能的切分路径(如上图所有从头至尾的不同路径)。对于分词，实际上就是求解使条件概率P(W∣S)最大的切分路径$W^*$，P(W∣S)即为每条路径的衡量标准。 W^* = argmax_W P(W|S)做贝叶斯公式转换: W^* = argmax_W \frac{P(W)P(S|W)}{P(S)}由于P(S)为归一化因子，而且一般情况下(词序列无论怎么变子序列都是不会变化)P(S∣W)为1，因此只需要求解 $P(W)$。假设我们使用 2-gram 语言模型进行建模(一个字词的出现只依赖它前面的那个字或词)。 P(W) = P(w_1w_2w_3...w_N) = P(w_1|s) \cdot P(w_2|w_1) \cdot P(w_3|w_2)... P(w_{N}|w_{N-1}) \cdot P(e|w_N)有了上述模型，就可以得到每条切分路径的$P(W|S)$也就是开头说的每条切分路径的衡量标准。在这只后再使用暴力穷举也好，动态规划也好，最短路径算法也好，最终找出最优解$W^*$序列即为最优的切词策略。 隐马尔科夫模型(HMM)切词 *再次强行插入一波广告隐马尔科夫模型的浅显介绍链接 字构词的分词方法思想，是将分词问题转化为字的分类问题（序列标注问题）。就是说序列标注有以下几种: 词首，词中，词尾，单子词等。举个下面的列子就能明白了。 研 究 生 说 的 确 实 在 理 词首 词中 词尾 单字词 单字词 词首 词尾 词首 词尾 当每个字的标注都得出的时候，切词也就顺理成章得完成了。 在隐马尔科夫模型中上述切词问题，对应以下几种要素，字为观察符号，标注为隐含状态，目标是找出概率最大的标注序列，实际上就是隐马尔科夫模型三大问题中的解码问题。 给定一个模型λ ， 和一个观察序列O，找出产生这一序列概率最大的状态序列Q这个问题可以使用Viterbi算法就可以轻松解决。 CRF切词条件随机场(CRF)同样也是一种标注思想，但与HMM不同的地方在于，HMM还算是生成式的话，CRF就属于判别式了，因为CRF主要的思想是将字进行分类任务得到它的最优的标注。CRF假设标注序列Y在给定观察序列X的条件下，Y构成的图为一个MRF(马尔科夫随机场)。 参考资料https://www.cnblogs.com/en-heng/p/6214023.htmlhttps://zhuanlan.zhihu.com/p/33261835]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型]]></title>
    <url>%2Farchives%2Fa66e40a.html</url>
    <content type="text"><![CDATA[隐马尔科夫模型是一种统计模型，他由马尔科夫随机过程生成无法观察的状态随机序列，每个状态又会生成不同的观测值，从而产生随机观测序列。 隐马尔可夫模型隐马尔科夫模型(HMM)是一种统计模型，他由马尔可夫随机过程生成无法观察的状态随机序列，每个状态又会生成不同的观测值，从而产生随机观测序列。因为状态序列是隐含的并且符合马尔科夫过程因此称为隐马尔可夫模型。他有三个基本问题，处理好这三个基本问题，基本就能熟练运用隐马尔可夫模型。概率计算问题: 能够对某个观测序列给出其发生的概率；学习问题: 通过一个观测序列，调整(学习)参数使得这个观测序列发生概率最大；(最大似然)编码问题: 通过一个观测序列，给出隐含的概率最大的状态序列 离散马尔可夫过程首先什么是马尔可夫过程。假设有N个不同的状态 ${S1,S_2,…S_N}$ 的随机过程。$a{ij}$ 代表 $S_i$ 到 $S_j$ 的转移概率。当这个随机过程满足：当前的状态仅与它之前的状态有关时，就是马尔可夫过程。用公式表达即： P[q_t=S_i|q_{t-1}=s_j,q_{t-2}=s_k,...] = P[q_t=s_i|q_{t-1}=s_j]于是得到转移概率： a_{ij}=P[q_t=s_i|q_{t-1}=s_j]其中转移概率$a{ij}$自然有两个属性：$a{ij}\geq0$和$\sum{j=1}^{N}a_{ij}=1$ 以上就是离散马尔科夫随机过程。 隐马尔科夫要素隐马尔可夫有五个要素组成： $N$ ，表示模型中的状态数（隐式状态）。模型中的各个状态是相互连结的，任何状态能从其它状态到达。我们用 $S$ 表示各个状态的集合，$S={s_1,s_2,…,s_N}$， $q_t$ 表示 t 时刻的状态。 $M$ ，表示模型中每个状态不同的观察集合(可见状态)数量。我们用$V$表示各个观察值(可见状态)集合，$V={v_1,v_2,…,v_M}$ $A$ ，状态转移概率分布 $B$ ，观察字符在状态j时的概率分布，$B={b_j(k)}$，其中$b_j(k)=P[v_k|q_t=S_j]$ $\pi$ ，表示状态分布，$\pi={\pi_j}$，其中$\pi_j=P[q_1=S_j]$ 给定$N,M,A,B,\pi$，HMMs能输出一个观察序列$O=O_1O_2…O_T$，其中$O_t\in V$ ，$T$ 是观察序列的数量。 从以上的讨论可知，一个完整的隐马尔可夫模型要求两个具体的模型参数 $N$ 和 $M$，和三个概率矩阵 $A,B,π$，也即隐马尔可夫模型可形式化定义为一个五元组$(N,M,A,B,π)$。 隐马尔可夫的三个基本问题 概率计算问题：给定一个模型$\lambda=(N,M,A,B,\pi)$，如何高效地计算某一输出观察序列$O=O_1O_2…O_T$的概率$P(O|\lambda)$ 编码问题：给定一个模型$\lambda=(N,M,A,B,\pi)$，和一个输出观察序列$O=O_1O_2…O_T$，如果找到产生这一序列概率最大的状态序列$Q=s_is_j…s_T$ 学习问题：给定一个模型$\lambda=(N,M,A,B,\pi)$，和一个输出观察序列$O=O_1O_2…O_T$，如何调整参数使得产生这一序列概率最大。 为了方便分析问题和给出解决方案，这里先介绍一下隐马尔可夫模型的条件独立性假设。隐马可尔可夫模型是一个生成模型，给定一个观察序列，HMMs 模型隐含一个与观察序列对应的状态序列。隐马可夫模型图示如下，图中清楚的表示出了隐马尔可夫模型内部的条件独立关系，有三个独立性假设：（一）t时刻的状态$q_t=s_i$只依赖于t-1时刻的状态。（二）t时刻所生成的值$b_i(O_t)$只依赖于t时刻的状态。（三）状态与具体时间无关。 了解这三个问题之后就已经知道HMM能够干什么，能够处理什么问题。这就足够了。 以下是解决三个问题的算法 问题1的暴力解法枚举所有的状态序列I（长度为T） P(Q|\lambda)=\pi_{q_1}\cdot a_{q_1q_2}\cdot...a_{q_{T-1}q_{T}} P(O|Q,\lambda)=b_{I_1O_1}\cdot b_{I_2O_2}...b_{I_{T-1}O_T} P(O,Q|\lambda)=P(O|I,\lambda)\cdot P(I|\lambda) P(O|\lambda)=\sum_I P(O,Q|\lambda)复杂度为 $O(2T\cdot N^T)$ 基本上无法完成 forward-backward 算法问题1的解法： 前向算法假设：$\alpha_t(i)=P(o_1o_2…o_t,q_t=s_i|\lambda)$ 那么就会有： 初值：$\alpha1(i)=\pi_ib{io_1}$ 递推：对于t=1,2…T-1 \alpha_{t+1}(j)=(\sum_{i=1}^N \alpha_t(i)a_{ij})b_{jO_{t+1}} 最终：$P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)$ 后向算法假设：$\betat(i)=P(o{t+1}o_{t+2}…o_T|q_t=s_i,\lambda)$ 初值：$\beta_T(i)=1$ 那么递推就会有： \beta_{t}(j)=\sum_{i=1}^N a_{ji}b_{iO_{t+1}}\beta_{t+1}(i) 最终：$P(O|\lambda)=\sum{i=1}^N\pi_ib{iO1}\beta_1(i)$ 至此前向后向算法就描述完了，其实我个人比较难理解后向算法。 Viterbi 算法问题2的解法：问题 2 是一个解码问题，即从 $N^T$ 个可能的状态序列中找到一个”最优”的状态序列。 Viterbi 算法：给定观察序列 $Ο=Ο_1Ο_2..Ο_T$，利用 Viterbi 动态规划算法可以有效率的找到一个最优的状态序列 $Q=q_1q_2…q_T$，计算量为 $NT^2$，如下图大致也能明白利用动态规划来找到一个最优的序列。 假设：$\delta_t(i)=max(P(o_1o_2…o_t,q_t=s_i|\lambda))$初值 : $\delta_1(i)=\pi_ib_i(o_1)$ 递推： \delta_{t+1}(i) = max[\delta_t(j)a_{ji}]b_i(o_t)Baum-welch算法问题3的解法：使用Baum-welch算法进行最大似然估计。这部分稍微有点复杂自身还未明白。就不在这里贴了，大家感兴趣可以在下文参考资料里找到想要的。 参考资料https://www.cnblogs.com/shixiangwan/p/8979106.htmlhttps://www.cnblogs.com/skyme/p/4651331.html]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日乘数法笔记]]></title>
    <url>%2Farchives%2F2456c8ba.html</url>
    <content type="text"><![CDATA[在求解函数最优化问题中，拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件是两种最常用的方法。函数有等式约束时使用拉格朗日乘子法，函数有不等约束时使用KKT条件。本文简要的复习下拉格朗日乘数法的浅层次问题。 本文简书链接 拉格朗日乘数法 在求解函数最优化问题中，拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件是两种最常用的方法。函数有等式约束时使用拉格朗日乘子法，函数有不等约束时使用KKT条件。 本篇文章主要整理拉格朗日乘子法这种方法可以将一个有 n 个变量与 k 个约束条件的最优化问题转换为一个解有 n + k 个变量的方程组的解的问题。这种方法中引入了一个或一组新的未知数，即拉格朗日乘数，又称拉格朗日乘子，或拉氏乘子，它们是在转换后的方程，即约束方程中作为梯度（gradient）的线性组合中各个向量的系数。 总结伸手党，比如两个变量求最优时，求 $f(x, y)$ 在条件 $ g(x,y)=c $ 时的最大值，我们可以引入新变量拉格朗日乘数 $ \lambda $，这时我们只需要下列拉格朗日函数的极值,此时就回归到了无约束时的最值问题： F(x,y,\lambda) = f(x,y)+\lambda \cdot(g(x,y)-c)无约束时函数最优问题这种问题，通常的解决办法是，对各变量求偏导，使得各偏导同时为零得到驻点。再判断驻点是否为极值点，最后代入原函数验证最优。 等式约束时函数最优问题设目标函数为 $f(x,y)$， 约束条件为 $g(x,y)=c$。问题是如何在满足约束条件的情况下，使得目标函数最大(最小)。 使用一个例子来描述这个问题：在双曲线 xy=3 的情况下，哪个点离原点最近。 f(x,y)=x^2+y^2 g(x,y)=xy=3如图： 那么f(x,y) 可以被描述为无数个一圈圈的等高线(图中所有颜色的圆圈线)，这些等高线与双曲线相交的点是满足约束条件的点。那么离原点最近的点，就是等高线与双曲线互切处的点，如图中，红色的点。 绿色的等高线无法与双曲线相交，没有满足约束条件的解 蓝色点虽然满足约束条件，但并非最优解 只有等高线与双曲线相切的红色的点，才是最优解。 在取最优解时，我们发现只有相切才能取最优解。那么如何判断相切呢？ 那就使用梯度向量（如图中红色蓝色的梯度向量），如果两者梯度向量互相平行时，那么两条曲线相切。于是引入一个参数 $\lambda$ 使得满足如下梯度公式： \nabla f(x,y) = \lambda \cdot \nabla g(x, y)那么原目标函数 f(x,y) 和 约束条件 g(x,y) ，在取最优值时满足上述公式那么： \begin{cases} f'_x = \lambda \cdot g'_x\\ f'_y = \lambda \cdot g'_y \end{cases}此时我们就拥有了三个公式，三个未知数的多项式。把三个未知数全部解出来。代入原目标函数就是函数最值。 最后我们稍微整理下这三条公式: \begin{cases} f'_x - \lambda \cdot g'_x = 0 \\\\ f'_y - \lambda \cdot g'_y = 0 \\\\ g(x,y)=c \end{cases}发现原最优问题可以被替换成求$F(x,y,\lambda) = f(x,y)+\lambda \cdot(g(x,y)-c)$的最优问题。且这个问题不受g(x,y)所约束，因此可以使用无约束时函数最优问题来解决。至此呼应了开头伸手党结论。]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python版本共存和虚拟环境]]></title>
    <url>%2Farchives%2Fe3ad352f.html</url>
    <content type="text"><![CDATA[python有很多版本，如何在同一台系统中共存这些版本而且不会冲突，是python开发者一个重要的需求，所以诞生出了Python版本管理器pyenv。另外每个Python项目都会有很多包需要导入，而另一些项目并不需要，如果所有包都加入到系统Python的包目录下的话一方面不好管理另一方面容易出现包的冲突，因此需要为每个项目建立一个虚拟的独立的Python环境就太好了，于是virtualenv和conda工具就出现了。 python 版本2.7.X3.XAnaconda2Anaconda3python有很多版本，如何在同一台系统中共存这些版本而且不会冲突，是python开发者一个重要的需求，所以诞生出了Python版本管理器pyenv。另外每个Python项目都会有很多包需要导入，而另一些项目并不需要，如果所有包都加入到系统Python的包目录下的话一方面不好管理另一方面容易出现包的冲突，因此需要为每个项目建立一个虚拟的独立的Python环境就太好了，于是virtualenv和conda工具就出现了。 pyenvpyenv是一个管理各个python版本的管理器。可以在系统里同时保留多个python版本，等需要时定义需要的版本。 项目地址 安装看项目地址中的readme 使用1pyenv install --list 查看pyenv可安装的版本列表 12pyenv install anaconda2-4.2.0pyenv uninstall x.x.x 安装和卸载指定版本，会将python版本安装在$(pyenv root)/versions/中 1pyenv versions 查看当前已经安装了的python版本。输出内容中，system关键字是系统python版本。 *表示当前环境所处的版本。 1pyenv global anaconda2 全局切换为anaconda科学计算环境(不建议这么做)，做了如果要恢复，则将最后一个参数改为—unset 1pyenv local python3.4.1 当前环境接环。在当前目录以下。如果要恢复，则将最后一个参数改为—unset virtualenv本来这是一个单独的软件用来虚拟一个python版本环境，让每个工作环境都有一套独立的python各自的第三方插件互不影响。然而在 pyenv 下有一个插件 pyenv-virtualenv 他可以在 pyenv 的环境下担负起 virtualenv 的事情。（如果使用的是原生python可以用这个工具，如果用的是anaconda则不用这个，用下一章说的conda工具来完成虚拟环境） 项目地址 安装看项目地址中的readme 使用1pyenv virtualenv 2.7.1 env271 在当前目录下创建一个 python 版本为2.7.1的环境，环境名字为 env271。 这个环境的真实目录位于~/.pyenv/versions/ 1pyenv activate env271 （创建时并不激活）激活当前环境。此时已经进入虚拟环境，在当前环境下所有pip等操作都不会影响系统环境和系统路径。 1pyenv deactivate 离开已激活的环境，切换回系统环境。但并没有被删除，下次依旧可以启动。 1pyenv uninstall env271 删除一个环境，当然也可以到真实目录下删除文件夹。 conda本来不想用这个的，但是因为 pyenv-virtualenv 有一些问题，无法很好的管理conda环境，比如有一些anaconda 自带的一些命令(例如pylint)无法被使用。因此还是老老实实使用 conda 来管理虚拟环境。 安装conda 是自带于 anaconda 的所以并不需要额外安装，如果在 anaconda 环境中就可以使用。conda 不仅可以进行 环境管理，还可以包管理，和对 anaconda和conda 进行版本升级。 使用由于conda使用方法太多，因此这里罗列一些常用的主要是一些虚拟环境的命令。具体的到官网文档去查看一下。 首先conda工具是需要在anaconda环境下的，因此先执行pyenv local anaconda3-4.2.0进入anaconda环境后就可以执行conda工具了。 12conda create --name myflakes python=x.x anacondaconda create --help 创建一个虚拟环境。可以指定名字，指定包，甚至制定python(这样的话就python版本管理了所以不建议使用，python版本管理交给pyenv)，所以命令中 python=x.x 可以不写 12conda info --envsconda env list 罗列已经创建的环境，两条命令是一样的。 12source activate myflakes //Linux,OSXactivate myflakes //Windows 激活一个环境。和virtualenv一样，创建不等于激活。激活后才能真正使用虚拟环境。如果发生错误Error: activate must be sourced. Run &#39;source activate envname&#39;说明activate命令没有找对，导致错误。吧命令改成1source &lt;PATH TO ANACONDA&gt;/anaconda3/bin/activate &lt;ENV NAME&gt; 这样就能成功建立虚拟环境了。 12deactivate myflakessource deactivate myflakes 退出虚拟环境。 1conda remove --name myflakes --all 删除一个虚拟环境。 1conda create --name flowers --clone snowflakes 复制一个虚拟环境。这个是个不错的好功能。 12conda env export &gt; environment.ymlconda env create -f environment.yml conda还能吧环境配置文件导出，在另一台机器上重新读入配置文件，就能复刻你的环境了。 其他使用conda 还能进行包的管理。调用的是 pip 所以也很棒。 123conda listconda list -n myflakesconda search numpy //查询package 查看当前环境已安装包，用-n指定后，就是查看某个环境下的已安装包 123conda install -n myflakes numpyconda update -n myflakes numpyconda remove -n myflakes numpy 为某个指定的环境安装包，升级包，删除包。 123conda update condaconda update anacondaconda update python 它还能升级自身和anaconda和python的版本。 它和pip一样也能设置安装包的镜像位置。其余还有能使用R命令等等，都到官网文档中搜索一下。]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10. Regular Expression Matching]]></title>
    <url>%2Farchives%2F6358f960.html</url>
    <content type="text"><![CDATA[leetcode #10 Regular Expression Matching 题目详解, 看网上帖子说的不清楚都是直接给代码也没个例子，所以起个帖子记录下自己的思路。 本文主要讲述递归解法和动态规划解法的思想。 10. Regular Expression Matching原文要求如下： Implement regular expression matching with support for &#39;.&#39; and &#39;*&#39;.1234567891011121314151617&apos;.&apos; Matches any single character.&apos;*&apos; Matches zero or more of the preceding element.The matching should cover the **entire** input string (not partial).The function prototype should be:bool isMatch(const char *s, const char *p)Some examples:isMatch(&quot;aa&quot;,&quot;a&quot;) → falseisMatch(&quot;aa&quot;,&quot;aa&quot;) → trueisMatch(&quot;aaa&quot;,&quot;aa&quot;) → falseisMatch(&quot;aa&quot;, &quot;a*&quot;) → trueisMatch(&quot;aa&quot;, &quot;.*&quot;) → trueisMatch(&quot;ab&quot;, &quot;.*&quot;) → trueisMatch(&quot;aab&quot;, &quot;c*a*b&quot;) → trueisMatch(&quot;aaaa&quot;,&quot;ab*a*c*a&quot;) → true #自己加的 解法有两种递归递归想法比较简单代码也比较清楚，但耗时很长复杂度是指数级别考虑递归思想时，只需要考虑终结情况，和当前情况，其他的任由递归完成当前情况的考虑如下：因为*号是最复杂的情况甚至可以发生0次于是分成两种情况分别考虑：p[1] 为 * 和 p[1] 不为 * p[1] != * 时：说明当前p[0]不会发生0-n次的变化直接对比就可以，剩下的交给递归 s[0] == p[0] 判等（这个相等包含了.的情况，后面相同） 递归判断 isMatch(s[1:],p[1:]) 两个子串 p[1] == * 时：说明当前p[0]会发生0-n次的变化，而且都是有效的 假设发生 0次 ：那么直接将p[0,1]跳过进行递归 isMatch(s,p[2:]) 假设发生 1次以上：那么得先判等，然后s+1进行递归 s[0]==p[0] and isMatch(s[1:],p) 好了所有情况都考虑好了，就可以直接上代码了，递归思想还是比较简单，直接看代码可能比看上述文字更加简单直接。 talk is cheep show me code： 1234567891011121314class Solution(object): def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ if p=="": return s=="" if len(p)&gt;1 and '*' == p[1]: return self.isMatch(s, p[2:]) or ((s!="" and (s[0]==p[0] or '.'==p[0])) and self.isMatch(s[1:], p)) else: return (s!="" and (s[0]==p[0] or '.'==p[0])) and self.isMatch(s[1:], p[1:]) 动态规划动态规划处理这个问题，更加有效，复杂度为 O(N*M).但是不同于递归直接看代码，动态规划我简直还是先看状态公式更加明了。用dp[i][j]来代表 s[0:i] 与 p[0:j] 是否匹配，初始化 dp[0][0]=1(空串匹配空串) dp[i][j]= \begin{cases} dp[i][j-1], & p[j-1]为\* 考虑到\*只重复1次 \\\\dp[i][j-2], & p[j-1]为\* 考虑到\*只重复0次 \\\\dp[i-1][j]\;\&\&\;S[i-1]==P[j-2], & p[j-1]为\* 考虑到\*只重复多次 \\\\dp[i-1][j-1] \;\&\&\; S[i-1]=P[j-1],& p[j-1]不为\* \end{cases}从状态公式基本也能看的明白，要计算 dp[i][j] 的值，要分成两个情况，两个情况分别处理后就能将dp填满，则最后的结果就是 dp[len(s)][len(p)]的值 如果看公式还有点不清楚，来举个栗子：s=&quot;ccd&quot; , p=&quot;a*c*d&quot;dp的矩阵情况如下： # ^ a * c * d ^ 1 0 1 0 1 0 c 0 0 0 1 1 0 c 0 0 0 0 1 0 d 0 0 0 0 0 1 蓝色那个是因为 dp[i][j-1]=1 所以这个*只重复1的匹配结果，因此可以为 1红色那个是因为 dp[i-1][j]=1 &amp;&amp; s[i-1]==p[j-2] 代表已经被重复过了，不止1次，但依然可以被继续重复下去 talk is cheep show me code： 1234567891011121314151617181920212223class Solution(object): def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ lens = len(s) lenp = len(p) dp = [[False for col in range(lenp+1)] for row in range(lens+1)] dp[0][0] = True for j in range(1, lenp+1): dp[0][j] = p[j-1]=='*' and dp[0][j-2]==1 for i in range(1, lens+1): for j in range(1, lenp+1): if p[j-1] == '*': dp[i][j] = dp[i][j-2] or dp[i][j-1] or (dp[i-1][j] and (s[i-1]==p[j-2] or '.'==p[j-2])) else: dp[i][j] = dp[i-1][j-1] and (s[i-1]==p[j-1] or '.'==p[j-1]) return dp[lens][lenp]]]></content>
  </entry>
  <entry>
    <title><![CDATA[python logging日志模块以及多进程日志]]></title>
    <url>%2Farchives%2Ffe118cd8.html</url>
    <content type="text"><![CDATA[本篇文章主要对 python logging 的介绍加深理解。更主要是 讨论在多进程环境下如何使用logging 来输出日志， 如何安全地切分日志文件。 本文简书链接 1. logging日志模块介绍python的logging模块提供了灵活的标准模块，使得任何Python程序都可以使用这个第三方模块来实现日志记录。python logging 官方文档 logging框架中主要由四个部分组成： Loggers: 可供程序直接调用的接口 Handlers: 决定将日志记录分配至正确的目的地 Filters: 提供更细粒度的日志是否输出的判断 Formatters: 制定最终记录打印的格式布局 2. logging的组成loggersloggers 就是程序可以直接调用的一个日志接口，可以直接向logger写入日志信息。logger并不是直接实例化使用的，而是通过logging.getLogger(name)来获取对象，事实上logger对象是单例模式，logging是多线程安全的，也就是无论程序中哪里需要打日志获取到的logger对象都是同一个。但是不幸的是logger并不支持多进程，这个在后面的章节再解释，并给出一些解决方案。 【注意】loggers对象是有父子关系的，当没有父logger对象时它的父对象是root，当拥有父对象时父子关系会被修正。举个例子logging.getLogger(&quot;abc.xyz&quot;)会创建两个logger对象，一个是abc父对象，一个是xyz子对象，同时abc没有父对象所以它的父对象是root。但是实际上abc是一个占位对象（虚的日志对象），可以没有handler来处理日志。但是root不是占位对象，如果某一个日志对象打日志时，它的父对象会同时收到日志，所以有些使用者发现创建了一个logger对象时会打两遍日志，就是因为他创建的logger打了一遍日志，同时root对象也打了一遍日志。 每个logger都有一个日志的级别。logging中定义了如下级别 Level Numeric value NOTSET 0 DEBUG 10 INFO 20 WARNING 30 ERROR 40 CRITICAL 50 当一个logger收到日志信息后先判断是否符合level，如果决定要处理就将信息传递给Handlers进行处理。 HandlersHandlers 将logger发过来的信息进行准确地分配，送往正确的地方。举个栗子，送往控制台或者文件或者both或者其他地方(进程管道之类的)。它决定了每个日志的行为，是之后需要配置的重点区域。 每个Handler同样有一个日志级别，一个logger可以拥有多个handler也就是说logger可以根据不同的日志级别将日志传递给不同的handler。当然也可以相同的级别传递给多个handlers这就根据需求来灵活的设置了。 FiltersFilters 提供了更细粒度的判断，来决定日志是否需要打印。原则上handler获得一个日志就必定会根据级别被统一处理，但是如果handler拥有一个Filter可以对日志进行额外的处理和判断。例如Filter能够对来自特定源的日志进行拦截or修改甚至修改其日志级别（修改后再进行级别判断）。 logger和handler都可以安装filter甚至可以安装多个filter串联起来。 FormattersFormatters 指定了最终某条记录打印的格式布局。Formatter会将传递来的信息拼接成一条具体的字符串，默认情况下Format只会将信息%(message)s直接打印出来。Format中有一些自带的LogRecord属性可以使用，如下表格: Attribute Format Description asctime %(asctime)s 将日志的时间构造成可读的形式，默认情况下是‘2016-02-08 12:00:00,123’精确到毫秒 filename %(filename)s 包含path的文件名 funcName %(funcName)s 由哪个function发出的log levelname %(levelname)s 日志的最终等级（被filter修改后的） message %(message)s 日志信息 lineno %(lineno)d 当前日志的行号 pathname %(pathname)s 完整路径 process %(process)s 当前进程 thread %(thread)s 当前线程 一个Handler只能拥有一个Formatter 因此如果要实现多种格式的输出只能用多个Handler来实现。 3. logging 配置简易配置首先在 loggers 章节里说明了一点，我们拥有一个缺省的日志对象root，这个root日志对象的好处是我们直接可以使用logging来进行配置和打日志。例如： 12logging.basicConfig(level=logging.INFO,filename='logger.log')logging.info("info message") 所以这里的简易配置所指的就是root日志对象，随拿随用。每个logger都是单例对象所以配置过一遍之后程序内任何地方调用都可以。我们只需要调用basicConfig就可以对root日志对象进行简易的配置，事实上这种方式相当有效易用。它使得调用任何logger时保证至少一定会有一个Handler能够处理日志。简易配置大致可以这么设置： 12345logging.basicConfig(level=logging.INFO, format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', datefmt='[%Y-%m_%d %H:%M:%S]', filename='../log/my.log', filemode='a') 代码配置另一种更加细致地设置方式是在代码中配置，但这种设置方式是使用的最少的方式，毕竟谁也不希望把设置写死到代码里面去。但是这里也稍微介绍一下，虽然用的不多，在必要的时候也可以用一把。(以后补上) 配置文件配置python中logging的配置文件是基于ConfigParser的功能。也就是说配置文件的格式也是按照这种方式来编写。先奉上一个比较一般的配置文件再细说 123456789101112131415161718192021222324252627282930313233343536373839##############################################[loggers]keys=root, log02[logger_root]level=INFOhandlers=handler01[logger_log02]level=DEBUGhandler=handler02qualname=log02##############################################[handlers]keys=handler01,handler02[handler_handler01]class=FileHandlerlevel=INFOformatter=form01args=(&apos;../log/cv_parser_gm_server.log&apos;,&quot;a&quot;)[handler_handler02]class=StreamHandlerlevel=NOTSETformatter=form01args=(sys.stdout,)##############################################[formatters]keys=form01,form02[formatter_form01]format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(process)d %(message)sdatefmt=[%Y-%m-%d %H:%M:%S][formatter_form02]format=(message)s############################################## 相信看一遍以后，也找出规律了，我将几个大块用#分了出来。每一个logger或者handler或者formatter都有一个key名字。以logger为例，首先需要在[loggers]配置中加上key名字代表了这个logger。然后用[loggers_xxxx]其中xxxx为key名来具体配置这个logger，在log02中我配置了level和一个handler名，当然你可以配置多个hander。根据这个handler名再去 [handlers]里面去找具体handler的配置，以此类推。然后在代码中，这样加载配置文件即可： 1logging.config.fileConfig(log_conf_file) 在handler中有一个class配置，可能有些读者并不是很懂。其实这个是logging里面原先就写好的一些handler类，你可以在这里直接调用。class指向的类相当于具体处理的Handler的执行者。在logging的文档中可以知道这里所有的Handler类都是线程安全的，大家可以放心使用。那么问题就来了，如果多进程怎么办呢。在下一章我主要就是重写Handler类，来实现在多进程环境下使用logging。 我们自己重写或者全部新建一个Handler类，然后将class配置指向自己的Handler类就可以加载自己重写的Handler了。 4. logging遇到多进程（important）这部分其实是我写这篇文章的初衷。python中由于某种历史原因，多线程的性能基本可以无视。所以一般情况下python要实现并行操作或者并行计算的时候都是使用多进程。但是 python 中logging 并不支持多进程，所以会遇到不少麻烦。 本次就以 TimedRotatingFileHandler 这个类的问题作为例子。这个Handler本来的作用是：按天切割日志文件。（当天的文件是xxxx.log 昨天的文件是xxxx.log.2016-06-01）。这样的好处是，一来可以按天来查找日志，二来可以让日志文件不至于非常大, 过期日志也可以按天删除。但是问题来了，如果是用多进程来输出日志，则只有一个进程会切换，其他进程会在原来的文件中继续打，还有可能某些进程切换的时候早就有别的进程在新的日志文件里打入东西了，那么他会无情删掉之，再建立新的日志文件。反正将会很乱很乱，完全没法开心的玩耍。所以这里就想了几个办法来解决多进程logging问题 原因在解决之前，我们先看看为什么会导致这样的原因。先将 TimedRotatingFileHandler 的源代码贴上来，这部分是切换时所作的操作：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def doRollover(self): """ do a rollover; in this case, a date/time stamp is appended to the filename when the rollover happens. However, you want the file to be named for the start of the interval, not the current time. If there is a backup count, then we have to get a list of matching filenames, sort them and remove the one with the oldest suffix. """ if self.stream: self.stream.close() self.stream = None # get the time that this sequence started at and make it a TimeTuple currentTime = int(time.time()) dstNow = time.localtime(currentTime)[-1] t = self.rolloverAt - self.interval if self.utc: timeTuple = time.gmtime(t) else: timeTuple = time.localtime(t) dstThen = timeTuple[-1] if dstNow != dstThen: if dstNow: addend = 3600 else: addend = -3600 timeTuple = time.localtime(t + addend) dfn = self.baseFilename + "." + time.strftime(self.suffix, timeTuple) if os.path.exists(dfn): os.remove(dfn) # Issue 18940: A file may not have been created if delay is True. if os.path.exists(self.baseFilename): os.rename(self.baseFilename, dfn) if self.backupCount &gt; 0: for s in self.getFilesToDelete(): os.remove(s) if not self.delay: self.stream = self._open() newRolloverAt = self.computeRollover(currentTime) while newRolloverAt &lt;= currentTime: newRolloverAt = newRolloverAt + self.interval #If DST changes and midnight or weekly rollover, adjust for this. if (self.when == 'MIDNIGHT' or self.when.startswith('W')) and not self.utc: dstAtRollover = time.localtime(newRolloverAt)[-1] if dstNow != dstAtRollover: if not dstNow: # DST kicks in before next rollover, so we need to deduct an hour addend = -3600 else: # DST bows out before next rollover, so we need to add an hour addend = 3600 newRolloverAt += addend self.rolloverAt = newRolloverAt 我们观察 if os.path.exists(dfn) 这一行开始，这里的逻辑是如果 dfn 这个文件存在，则要先删除掉它，然后将 baseFilename 这个文件重命名为 dfn 文件。然后再重新打开 baseFilename这个文件开始写入东西。那么这里的逻辑就很清楚了 假设当前日志文件名为 current.log 切分后的文件名为 current.log.2016-06-01 判断 current.log.2016-06-01 是否存在，如果存在就删除 将当前的日志文件名 改名为current.log.2016-06-01 重新打开新文件（我观察到源代码中默认是”a” 模式打开，之前据说是”w”） 于是在多进程的情况下，一个进程切换了，其他进程的句柄还在 current.log.2016-06-01 还会继续往里面写东西。又或者一个进程执行切换了，会把之前别的进程重命名的 current.log.2016-06-01 文件直接删除。又或者还有一个情况，当一个进程在写东西，另一个进程已经在切换了，会造成不可预估的情况发生。还有一种情况两个进程同时在切文件，第一个进程正在执行第3步，第二进程刚执行完第2步，然后第一个进程 完成了重命名但还没有新建一个新的 current.log 第二个进程开始重命名，此时第二个进程将会因为找不到 current 发生错误。如果第一个进程已经成功创建了 current.log 第二个进程会将这个空文件另存为 current.log.2016-06-01。那么不仅删除了日志文件，而且，进程一认为已经完成过切分了不会再切，而事实上他的句柄指向的是current.log.2016-06-01。好了这里看上去很复杂，实际上就是因为对于文件操作时，没有对多进程进行一些约束，而导致的问题。那么如何优雅地解决这个问题呢。我提出了两种方案，当然我会在下面提出更多可行的方案供大家尝试。 解决方案1先前我们发现 TimedRotatingFileHandler 中逻辑的缺陷。我们只需要稍微修改一下逻辑即可： 判断切分后的文件 current.log.2016-06-01 是否存在，如果不存在则进行重命名。（如果存在说明有其他进程切过了，我不用切了，换一下句柄即可） 以”a”模式 打开 current.log 发现修改后就这么简单~talking is cheap show me the code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class SafeRotatingFileHandler(TimedRotatingFileHandler): def __init__(self, filename, when='h', interval=1, backupCount=0, encoding=None, delay=False, utc=False): TimedRotatingFileHandler.__init__(self, filename, when, interval, backupCount, encoding, delay, utc) """ Override doRollover lines commanded by "##" is changed by cc """ def doRollover(self): """ do a rollover; in this case, a date/time stamp is appended to the filename when the rollover happens. However, you want the file to be named for the start of the interval, not the current time. If there is a backup count, then we have to get a list of matching filenames, sort them and remove the one with the oldest suffix. Override, 1. if dfn not exist then do rename 2. _open with "a" model """ if self.stream: self.stream.close() self.stream = None # get the time that this sequence started at and make it a TimeTuple currentTime = int(time.time()) dstNow = time.localtime(currentTime)[-1] t = self.rolloverAt - self.interval if self.utc: timeTuple = time.gmtime(t) else: timeTuple = time.localtime(t) dstThen = timeTuple[-1] if dstNow != dstThen: if dstNow: addend = 3600 else: addend = -3600 timeTuple = time.localtime(t + addend) dfn = self.baseFilename + "." + time.strftime(self.suffix, timeTuple)## if os.path.exists(dfn):## os.remove(dfn) # Issue 18940: A file may not have been created if delay is True.## if os.path.exists(self.baseFilename): if not os.path.exists(dfn) and os.path.exists(self.baseFilename): os.rename(self.baseFilename, dfn) if self.backupCount &gt; 0: for s in self.getFilesToDelete(): os.remove(s) if not self.delay: self.mode = "a" self.stream = self._open() newRolloverAt = self.computeRollover(currentTime) while newRolloverAt &lt;= currentTime: newRolloverAt = newRolloverAt + self.interval #If DST changes and midnight or weekly rollover, adjust for this. if (self.when == 'MIDNIGHT' or self.when.startswith('W')) and not self.utc: dstAtRollover = time.localtime(newRolloverAt)[-1] if dstNow != dstAtRollover: if not dstNow: # DST kicks in before next rollover, so we need to deduct an hour addend = -3600 else: # DST bows out before next rollover, so we need to add an hour addend = 3600 newRolloverAt += addend self.rolloverAt = newRolloverAt 不要以为代码那么长，其实修改部分就是 “##” 注释的地方而已，其他都是照抄源代码。这个类继承了 TimedRotatingFileHandler 重写了这个切分的过程。这个解决方案十分优雅，改换的地方非常少，也十分有效。但有网友提出，这里有一处地方依然不完美，就是rename的那一步，如果就是这么巧，同时两个或者多个进程进入了 if 语句，先后开始 rename 那么依然会发生删除掉日志的情况。确实这种情况确实会发生，由于切分文件一天才一次，正好切分的时候同时有两个Handler在操作，又正好同时走到这里，也是蛮巧的，但是为了完美，可以加上一个文件锁，if 之后加锁，得到锁之后再判断一次，再进行rename这种方式就完美了。代码就不贴了，涉及到锁代码，影响美观。 解决方案2我认为最简单有效的解决方案。重写FileHandler类（这个类是所有写入文件的Handler都需要继承的TimedRotatingFileHandler 就是继承的这个类；我们增加一些简单的判断和操作就可以。 我们的逻辑是这样的： 判断当前时间戳是否与指向的文件名是同一个时间 如果不是，则切换 指向的文件即可 结束，是不是很简单的逻辑。 talking is cheap show me the code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class SafeFileHandler(FileHandler): def __init__(self, filename, mode, encoding=None, delay=0): """ Use the specified filename for streamed logging """ if codecs is None: encoding = None FileHandler.__init__(self, filename, mode, encoding, delay) self.mode = mode self.encoding = encoding self.suffix = "%Y-%m-%d" self.suffix_time = "" def emit(self, record): """ Emit a record. Always check time """ try: if self.check_baseFilename(record): self.build_baseFilename() FileHandler.emit(self, record) except (KeyboardInterrupt, SystemExit): raise except: self.handleError(record) def check_baseFilename(self, record): """ Determine if builder should occur. record is not used, as we are just comparing times, but it is needed so the method signatures are the same """ timeTuple = time.localtime() if self.suffix_time != time.strftime(self.suffix, timeTuple) or not os.path.exists(self.baseFilename+'.'+self.suffix_time): return 1 else: return 0 def build_baseFilename(self): """ do builder; in this case, old time stamp is removed from filename and a new time stamp is append to the filename """ if self.stream: self.stream.close() self.stream = None # remove old suffix if self.suffix_time != "": index = self.baseFilename.find("."+self.suffix_time) if index == -1: index = self.baseFilename.rfind(".") self.baseFilename = self.baseFilename[:index] # add new suffix currentTimeTuple = time.localtime() self.suffix_time = time.strftime(self.suffix, currentTimeTuple) self.baseFilename = self.baseFilename + "." + self.suffix_time self.mode = 'a' if not self.delay: self.stream = self._open() check_baseFilename 就是执行逻辑1判断；build_baseFilename 就是执行逻辑2换句柄。就这么简单完成了。这种方案与之前不同的是，当前文件就是 current.log.2016-06-01 ，到了明天当前文件就是current.log.2016-06-02 没有重命名的情况，也没有删除的情况。十分简洁优雅。也能解决多进程的logging问题。 解决方案其他当然还有其他的解决方案，例如由一个logging进程统一打日志，其他进程将所有的日志内容打入logging进程管道由它来打理。还有将日志打入网络socket当中也是同样的道理。 5. 参考资料python logging 官方文档林中小灯的切分方案，方案一就是从这儿来的]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test Markdown]]></title>
    <url>%2Farchives%2F66cfa612.html</url>
    <content type="text"><![CDATA[列举一些常用的 markdown 语法, 作为测试以及备忘。 You can write regular markdown here and Jekyll will automatically convert it to a nice webpage. I strongly encourage you to take 5 minutes to learn how to write in markdown - it’ll teach you how to transform regular text into bold/italics/headings/tables/etc. Markdown 语法手册 1. 分级标题使用 === 表示一级标题，使用 —- 表示二级标题你也可以选择在行首加井号表示不同级别的标题 (H1-H6)，例如：# H1, ## H2, ### H3一般来说都是使用 #号 来使用, 注意 #号 需要顶格写示例： 12345这是一个一级标题===这是一个二级标题----### 这是一个三级标题 2. 斜体和粗体使用 *和 ** 表示斜体和粗体 示例： 这是*斜体* 这是**粗体** 这是斜体 这是粗体 这是斜体 这是粗体 3. 链接使用 [描述](链接地址) 为文字增加链接 示例： 这是去往[本人博客](http://doudou0o.github.com/blog) 的链接 这是去往 本人博客 的链接 4. 插入图像使用 ![描述](图片链接) 插入图像(好像插入时保持原图片大小)和链接比较类似,唯一的区别就是前面有一个!在Hexo 中本地图片使用 ![本地图片](/img/a.jpg) 示例：在线图片![图片](https://assets-cdn.github.com/images/modules/logos_page/Octocat.png) 本地图片![本地图片](./Test Markdown/start.jpg)![本地图片](./66cfa612/start.jpg) 大小控制图片1&lt;img src=&quot;./&#123;&#123; abbrlink &#125;&#125;/start.jpg&quot; alt=&quot;logo&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt; 5. 文字引用使用 &gt; 表示文字引用同 #号 一样需要定格写否则没有效果而且上下需要有空格 示例： 你好我是引用文档 6. 无序列表使用 *，+，- 表示无序列表都是需要顶格写且前后得有空行 示例： 无序列表项 一 无序列表项 二 无序列表项 三 7. 有序列表使用数字和点表示有序列表。例: 1. xxxx 注意要有一个空格! 示例： 有序列表项 一 有序列表项 二 有序列表项 三 8. 行内代码块使用 `代码` 表示行内代码块好像在行内代码块中不会高亮 示例：此行内有一个行内代码块java vs cpp 9. 代码块代码块使用 ``` python(或者~~~)balabala``` 包裹起来而且可以加上所用的语言,也可以不加另外由于 Pygments 或 Rouge 的使用,可以用 包裹更加好看(不要加行号! 不然会超级难看) def show @widget = Widget(params[:id]) respond_to do |format| format.html # show.html.erb format.json { render json: @widget } end end 10. 表格 Number Next number Previous number Five Six Four Ten Eleven Nine Seven Eight Six Two Three One 11. Todo 列表使用带有 [ ] 或 [x] （未完成或已完成）项的列表语法撰写一个待办事宜列表并且支持子列表嵌套以及混用Markdown语法，示例： - [ ] **买菜** - [ ] 小排 - [x] 鸡肉 - [ ] 番茄 - [x] 辣椒 - [x] 小辣椒 - [x] 红辣椒 生成 这样一个 Todo 列表： [ ] 买菜 [ ] 小排 [x] 鸡肉 [ ] 番茄 [x] 辣椒 [x] 小辣椒 [x] 红辣椒 12. 注脚使用 [^keyword] 表示注脚 这是一个注脚footnote的样例 13. 页内链接(hexo下暂时无效没找到正确的姿势)1234567先在某处设定一个锚点`&lt;a id=&quot;jump&quot; name=&quot;jump&quot;&gt;锚点&lt;/a&gt;`类似这样然后使用 `\[结论\](\#jump) `这样的方法就可以在页内跳转或者在标题后面跟上ID也可以同样跳转`### 标题 &#123;#ID&#125;`示例: [示例1](#jump) [示例2](#ID2) footnote. 这是一个 注脚 &#8617;]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 搭建博客]]></title>
    <url>%2Farchives%2Fb6159bfb.html</url>
    <content type="text"><![CDATA[倒腾了很久 Jekyll 之后，还是决定投入到 Hexo 阵营中来。Hexo的 markdown 编译器以及优秀的各种模板还有一键部署都是相当赞的功能。由于我也是新手白菜，记录本篇搭建日志，以防遗忘和换些功能或模板设置。 github 部分略过Hexo什么是HexoHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。Hexo 的文档还是比较简单明白的~这里是Hexo中文文档(https://hexo.io/zh-cn/docs/)这里是Hexo英文文档(https://hexo.io/docs/index.html) 安装 Hexo首先安装前，确认已经成功安装了 Node.js 和 git Node.js Git 然后目录下执行(windows 也是一样) 12$ npm install -g hexo-cli --save$ npm install --save #(忘了这步也没事，就是安装些插件之类的) 建立博客进入你要搭建博客的目录，执行以下命令，就能新建一些博客文件12$ cd &lt;folder&gt;$ hexo init 新建完成后，指定文件夹的目录如下：123456789.|├── _config.yml #配置文件├── package.json #应用程序的信息（其实我也不知道）├── scaffolds #模版文件夹├── source #资源文件夹| ├── _drafts #草稿| └── _posts #发布的文章└── themes #主题 Hexo 命令init1$ hexo init 新建一直网站，注意了，这里它会新建模板文件，新建一个HelloWorld.md，新建一个_config.yml。其他不变，所以轻易没备份这些内容钱不要敲这个命令 clean 、new 、 server123$ hexo clean #清除缓存文件 (`db.json`) 和已生成的静态文件 (`public`)。$ hexo new &lt;file1&gt; #新建一篇文章$ hexo server #启动服务器 generate 、 deploy12$ hexo generate #生成静态文件$ hexo deploy #部署网站 在部署前，在 _config.yml 中将自己的deploy设置配置好部署时发现 ERROR Deployer not found: git 则需要安装这个插件1npm install hexo-deployer-git --save 博客设置找几个自己喜欢的主题，根据主题的设置来进行配置。整个 Hexo 博客有两个 config 一个是 Hexo 的设置，另一个就是 主题下的 config基本上每个主题会有一定配置教程，注意其中的路径问题。 博客写作本地图片首先将 Hexo _config.yml 配置中将这个打开 post_asset_folder:true打开后，每次新建一篇文章会同时建立一个同名的文件夹，里面的资源可以直接访问如果没有找到，那么可能是插件未安装，执行以下命令1npm install https://github.com/CodeFalling/hexo-asset-image --save 于是在markdown中可以直接引用改资源，如下1![本地图片](./a.jpg) 本地预览一般情况下，直接修改，刷新页面就可以看见修改，不需要重启服务。但是有时候，必须要重启服务，而且先 clean 再 server 才能生效。 其他遇到问题再写其他如果说mathjax遇到渲染问题，先查看是否下划线有问题，原来的解释器会将下划线转义掉。处理办法： npm uninstall hexo-renderer-marked —savednpm install hexo-renderer-kramed —saved 另外还是mathjax遇到渲染问题，发现不支持大括号处理办法: 找到../node_modules/kramed/lib/rules/inline.js文件 1234//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 第11行，将其修改为escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 第20行，将其修改为em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 它取消了该渲染引擎对\,{,}的转义 如果遇到Error: Validation Failed的gitment错误则需要修改以下内容123layout/_third-party/comments/gitment.swig将 id: window.location.pathname,改为 id: &apos;&#123;&#123; page.title &#125;&#125;&apos;]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchives%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
